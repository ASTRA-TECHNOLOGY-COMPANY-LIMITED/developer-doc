# 대규모 언어 모델과 Hugging Face Transformer에 대한 이해

URL: https://www.youtube.com/watch?v=yS05Wr6h2TU

## 1. 대규모 언어 모델 (LLM)

예를 들어, 우리가 앵무새를 키운다고 가정해봅시다. 이 앵무새는 우리가 "나는 너무 배고파요, 나는 **빵**을 먹고 싶어요"라고 말하는 것을 들을 것입니다. 어느 날, 우리가 "나는 너무 배고파요, 나는 ... 먹고 싶어요"라고만 말하면, 앵무새는 자동으로 "빵"이라고 대답할 것입니다.

![Untitled](images/Untitled.png)

이것이 바로 **언어 모델**입니다. 이는 많은 훈련을 받아 우리가 프롬프트(미완성 문장)를 입력으로 제공하면, 그 프롬프트에 적합한 출력을 제공하는 것입니다.

앵무새가 우리가 말하는 내용을 이해하지 못하더라도, 매일 우리의 말을 듣기 때문에 가장 높은 확률의 단어로 우리의 문장을 완성할 수 있게 됩니다.

앵무새는 매일 자라면서 여기저기 날아다닙니다. 시간이 지나면서 다양한 사람들로부터 많은 문장을 듣게 되어 훨씬 더 복잡한 질문에 대답할 수 있게 됩니다.

이제 단순히 문장을 완성하는 것을 넘어 복잡한 질문에 대답할 수 있게 됩니다. 이 시점에서 앵무새는 **대규모 언어 모델**과 비슷해집니다 - 엄청난 양의 데이터로 훈련된 실체입니다.

![Untitled](images/Untitled%201.png)

**언어 모델**은 훈련된 데이터를 기반으로 단어를 생성할 수 있는 확률 모델입니다. 하지만 사실, 그것은 언어를 이해하지는 못합니다.

![Untitled](images/Untitled%202.png)

훈련 방식:

- 모델에게 대량의 정보를 학습시킵니다
- 충분히 학습하면 적절한 출력을 제공할 수 있습니다

다른 데이터 세트로 훈련된 모델들은 서로 다른 출력을 제공합니다.

![Untitled](images/Untitled%203.png)

**대규모 언어 모델 (LLM)**은 심층 학습을 사용하고 매우 큰 데이터 세트로 훈련되어 정보를 이해, 요약, 생성, 예측하는 알고리즘 유형입니다.

![Untitled](images/Untitled%204.png)

### 1.1. 왜 "대규모"라고 부르나요?

대규모라고 부르는 이유:

1. **매우 많은 수의 매개변수를 가짐** (신경망에 많은 층과 유닛이 있음; 매개변수는 유닛 간의 연결에서 생성됨)
    - GPT-3.5는 13억 개의 매개변수를 가짐 (나중에 1,750억 개로 증가)
    - GPT-4는 1.76조 개의 매개변수를 가짐
2. **매우 큰 데이터 세트로 훈련됨**
    - GPT-3.5는 45TB의 텍스트 데이터로 훈련됨

ChatGPT를 훈련시키려면 약 10,000대의 Nvidia GPU가 필요합니다.

### 1.2. LLM은 무엇을 할 수 있나요?

다음과 같은 여러 자연어 처리 작업을 수행할 수 있습니다:

- 언어 번역
- 텍스트 합성 및 요약
- 질문 답변
- 감정 분석
- 사용자의 입력과 맥락에 맞는 답변 생성

## 2. Transformers, Hugging Face

### 2.1. Transformer의 구조

![Untitled](images/Untitled%205.png)

Transformer는 LLM의 핵심 구조입니다.

왜 Transformer를 사용해야 할까요? 순환 신경망 구조도 자연어 처리를 할 수 있지 않나요?

![Untitled](images/Untitled%206.png)

- RNN 구조는 단어를 순차적으로 입력 → 병렬 훈련 불가능 → 시간이 오래 걸리고 GPU의 이점을 활용할 수 없음
- 긴 문장의 경우, 첫 단어와 마지막 단어 사이의 관계가 약해짐

따라서 우리는 Transformer 구조를 사용합니다. Transformer를 사용하면 전체 문장을 한 번에 네트워크에 입력하여 처리하고 출력할 수 있으며, RNN처럼 단어를 하나씩 입력할 필요가 없습니다.

![Untitled](images/Untitled%207.png)

단어들이 한 번에 입력되기 때문에 다음이 필요합니다:

- Positional Encoding: 어떤 단어가 먼저 들어왔고 어떤 단어가 나중에 들어왔는지 표시
- Attention과 Self-Attention: 문장 내의 단어들이 서로 연관될 수 있도록 도움

## 3. LLM의 응용 및 미세 조정

LLM의 응용 분야:

- 헬스케어
- 금융
- 법률
- 고객 서비스: 챗봇, 가상 비서
- 콘텐츠 제작
- 교육
- 엔터테인먼트 및 예술

하나의 LLM으로 우리가 필요로 하는 모든 작업과 분야에 사용할 수 있습니다. 하지만 **미세 조정**을 하면 더 좋은 효과를 얻을 수 있습니다.

**미세 조정**은 사전 훈련된 모델을 가져와 특정 분야에 대한 더 구체적인 데이터셋으로 훈련시키는 것을 말합니다 (이 데이터셋은 일반적으로 사전 훈련된 모델의 데이터셋보다 훨씬 작지만 특정 분야에 더 전문화되어 있습니다).

미세 조정에는 3가지 방법이 있습니다:

- 자기 지도 학습: 모델을 가져와 텍스트 데이터셋을 제공하여 스스로 학습하게 합니다. 일정 시간이 지나면 제공한 데이터셋과 유사한 문장을 생성할 수 있게 됩니다.
- 지도 학습: 모델을 가져와 (입력, 출력) 형태의 데이터셋을 제공합니다. 이 경우 출력은 질문-답변 형식과 유사하게 됩니다.
- 강화 학습: 복잡합니다.

지도 학습의 과정:

- 작업 선택
- 데이터셋 준비
- 기본 모델 선택
- 모델 미세 조정

미세 조정 시 3가지 선택지가 있습니다:

- 모든 매개변수 재훈련: 모든 매개변수를 다시 훈련시킵니다.
- 전이 학습: 마지막 몇 개 층만 훈련시킵니다.
- 매개변수 효율적 미세 조정 (PEFT) (또는 LoRA): 복잡합니다.

## 4. 기타 개념들

입력 크기: 4096 토큰 등

최대 길이: 1024 등

Temperature - LLM의 창의성 정도: 0.1 → 1.0