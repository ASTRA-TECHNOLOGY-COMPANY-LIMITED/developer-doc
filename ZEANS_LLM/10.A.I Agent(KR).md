# A.I 에이전트 개발 팁

## 목차

1. [효율적인 상태(State) 관리](#효율적인-상태-관리)
    - [가벼운 상태 관리의 중요성](#가벼운-상태-관리의-중요성)
2. [프롬프트 엔지니어링](#프롬프트-엔지니어링)
    - [명확한 지시 구조화](#명확한-지시-구조화)
    - [역할 기반 프롬프트 설계](#역할-기반-프롬프트-설계)
    - [컨텍스트 윈도우 최적화](#컨텍스트-윈도우-최적화)
    - [프롬프트 템플릿 및 변수화](#프롬프트-템플릿-및-변수화)
3. [RAG 활용 전략](#rag-활용-전략)
    - [Agentic RAG](#agentic-rag)
        - [핵심 개념](#핵심-개념)
        - [구현 예시](#구현-예시)
    - [Adaptive RAG](#adaptive-rag)
        - [핵심 개념](#핵심-개념-1)
        - [구현 예시](#구현-예시-1)
    - [Collaborative RAG](#collaborative-rag)
        - [핵심 개념](#핵심-개념-2)
        - [구현 예시](#구현-예시-2)
    - [Self-RAG](#self-rag)
        - [핵심 개념](#핵심-개념-3)
        - [구현 예시](#구현-예시-3)
4. [Plan & Execute 패턴](#plan--execute-패턴)
    - [핵심 개념](#핵심-개념-4)
    - [장점](#장점)
    - [구현 예시](#구현-예시-4)
    - [활용 팁](#활용-팁)

## 1. 효율적인 상태 관리

A.I 에이전트를 개발할 때 가장 중요한 고려사항 중 하나는 상태(State) 관리입니다. 메모리 사용량을 최소화하여 효율적인 에이전트를 구현하는 방법에 대해 알아보겠습니다.

### 1.1. 가벼운 상태 관리의 중요성

A.I 에이전트는 작업을 수행하는 동안 다양한 정보를 기억해야 합니다. 하지만 모든 정보를 상태로 저장하면 다음과 같은 문제가 발생합니다:

-   메모리 사용량 증가
-   처리 속도 저하
-   비용 증가 (특히 토큰 기반 모델 사용 시)
-   확장성 제한

따라서 상태를 가볍게 관리하는 것이 중요합니다. 이를 위한 몇 가지 전략은 다음과 같습니다:

1. **필수 정보만 저장하기**: 에이전트가 작업을 수행하는 데 정말 필요한 정보만 상태로 저장합니다.
2. **요약 기법 활용**: 대화 기록이나 문서 내용을 전체 저장하는 대신 핵심 요약본만 유지합니다.
3. **외부 저장소 활용**: 모든 정보를 상태로 관리하지 말고, 필요에 따라 외부 데이터베이스나 파일 시스템을 활용합니다.
4. **TTL(Time-To-Live) 설정**: 오래된 상태 정보는 자동으로 삭제되도록 설정합니다.
5. **증분식 상태 업데이트**: 전체 상태를 다시 계산하는 대신 변경된 부분만 업데이트합니다.

효율적인 상태 관리를 통해 A.I 에이전트의 성능을 향상시키고 자원 사용을 최적화할 수 있습니다.

## 2. 프롬프트 엔지니어링

A.I 에이전트의 성능은 프롬프트 설계에 크게 의존합니다. 효과적인 프롬프트 엔지니어링을 통해 에이전트의 성능을 크게 향상시킬 수 있습니다.

### 2.1. 명확한 지시 구조화

에이전트에게 제공하는 프롬프트는 명확하고 구조화된 형태여야 합니다:

-   목표와 제약 조건을 명확히 정의
-   단계별 지시사항 제공
-   기대하는 출력 형식 명시
-   경계 조건 및 예외 처리 방법 설명

### 2.2. 역할 기반 프롬프트 설계

에이전트에게 특정 역할을 부여하면 더 일관된 응답을 얻을 수 있습니다:

1. **전문가 페르소나 부여**: "당신은 수학 전문가입니다" 또는 "당신은 경험 많은 소프트웨어 엔지니어입니다"와 같은 역할 부여
2. **다중 페르소나 활용**: 복잡한 문제에서는 여러 전문가 관점을 가진 에이전트 활용
3. **책임과 권한 명시**: 에이전트가 수행해야 할 작업과 결정할 수 있는 범위를 명확히 지정

### 2.3. 컨텍스트 윈도우 최적화

대형 언어 모델은 제한된 컨텍스트 윈도우를 가지고 있어 효율적인 프롬프트 설계가 중요합니다:

-   **핵심 정보 우선 배치**: 가장 중요한 지시사항과 정보를 프롬프트 앞부분에 배치
-   **정보 압축**: 불필요한 세부 사항을 제거하고 핵심 내용만 포함
-   **구조화된 포맷 사용**: 마크다운, JSON 등의 형식을 활용해 정보를 구조화
-   **레퍼런스 활용**: 상세 내용은 외부 저장소에 보관하고 필요할 때 참조하도록 설계

### 2.4. 프롬프트 템플릿 및 변수화

재사용 가능한 프롬프트 템플릿을 구축하면 일관성을 유지하고 개발 효율성을 높일 수 있습니다:

1. **모듈화된 프롬프트**: 특정 기능별로 분리된 프롬프트 템플릿 구축
2. **변수 주입 메커니즘**: 동적 데이터를 쉽게 주입할 수 있는 템플릿 시스템 구현
3. **버전 관리**: 프롬프트 템플릿의 버전을 관리하여 성능 변화 추적

프롬프트 엔지니어링을 지속적으로 개선하고 테스트함으로써 A.I 에이전트의 성능, 신뢰성 및 사용자 경험을 향상시킬 수 있습니다.

## 3. RAG 활용 전략

RAG(Retrieval-Augmented Generation)는 A.I 에이전트가 외부 지식 소스를 효과적으로 활용할 수 있게 해주는 강력한 기법입니다. RAG를 효과적으로 구현하고 활용하는 전략에 대해 알아보겠습니다.

### 3.1. Agentic RAG

Agentic RAG는 에이전트가 질문을 분석하고, 필요한 정보를 검색하며, 답변을 생성하는 과정을 조율합니다.

#### 3.1.1. 핵심 개념

-   **질문 분석 및 분해**: 복잡한 질문을 작은 하위 질문으로 분해하여 더 효과적으로 정보를 검색합니다.
-   **관련 정보 검색**: 각 하위 질문에 필요한 정보를 벡터 데이터베이스나 외부 소스에서 검색합니다.
-   **답변 합성 및 검증**: 검색된 정보를 바탕으로 하위 질문에 답변하고, 이를 통합하여 최종 답변을 생성합니다.

#### 3.1.2. 구현 예시

다음은 LangGraph를 활용한 Agentic RAG 구현 예시입니다:

```python
from typing import TypedDict, Annotated, Sequence, List, Dict, Any
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.runnables import RunnablePassthrough
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI

# 상태 정의
class AgenticRAGState(TypedDict):
    question: str
    sub_questions: List[str]
    retrieved_documents: Dict[str, List[str]]
    answers: Dict[str, str]
    final_answer: str

# 노드 함수 정의
def analyze_question(state: AgenticRAGState):
    """질문을 분석하고 하위 질문으로 분해합니다."""
    llm = ChatOpenAI(model="gpt-4")
    response = llm.invoke(
        [HumanMessage(content=f"다음 질문을 필요한 하위 질문으로 분해해주세요: {state['question']}")]
    )
    # 간단한 예시: 실제로는 LLM 응답에서 하위 질문을 파싱해야 함
    sub_questions = ["데이터베이스란 무엇인가?", "관계형 데이터베이스의 종류는?", "NoSQL 데이터베이스의 장점은?"]
    return {"sub_questions": sub_questions}

def retrieve_documents(state: AgenticRAGState):
    """각 하위 질문에 대한 문서를 검색합니다."""
    # 실제 구현에서는 벡터 데이터베이스 검색 로직이 필요
    retrieved_docs = {}
    for question in state["sub_questions"]:
        # 벡터 검색 시뮬레이션
        retrieved_docs[question] = [f"Document for {question}"]
    return {"retrieved_documents": retrieved_docs}

def answer_sub_questions(state: AgenticRAGState):
    """각 하위 질문에 대해 검색된 문서를 바탕으로 답변합니다."""
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    answers = {}
    for question, docs in state["retrieved_documents"].items():
        context = "\n".join(docs)
        response = llm.invoke(
            [HumanMessage(content=f"컨텍스트: {context}\n\n질문: {question}")]
        )
        answers[question] = response.content
    return {"answers": answers}

def synthesize_final_answer(state: AgenticRAGState):
    """모든 하위 질문에 대한 답변을 통합하여 최종 답변을 생성합니다."""
    llm = ChatOpenAI(model="gpt-4")
    sub_answers = "\n".join([f"Q: {q}\nA: {a}" for q, a in state["answers"].items()])
    response = llm.invoke(
        [HumanMessage(content=f"다음 하위 질문과 답변을 바탕으로 '{state['question']}'에 대한 종합적인 답변을 작성해주세요:\n\n{sub_answers}")]
    )
    return {"final_answer": response.content}

# 그래프 구성
def build_agentic_rag_graph():
    workflow = StateGraph(AgenticRAGState)

    # 노드 추가
    workflow.add_node("analyze_question", analyze_question)
    workflow.add_node("retrieve_documents", retrieve_documents)
    workflow.add_node("answer_sub_questions", answer_sub_questions)
    workflow.add_node("synthesize_final_answer", synthesize_final_answer)

    # 엣지 정의
    workflow.add_edge("analyze_question", "retrieve_documents")
    workflow.add_edge("retrieve_documents", "answer_sub_questions")
    workflow.add_edge("answer_sub_questions", "synthesize_final_answer")
    workflow.add_edge("synthesize_final_answer", END)

    # 시작 노드 설정
    workflow.set_entry_point("analyze_question")

    return workflow.compile()

# 사용 예시
if __name__ == "__main__":
    graph = build_agentic_rag_graph()
    result = graph.invoke({
        "question": "데이터베이스의 종류와 각각의 특징을 설명해주세요."
    })
    print(result["final_answer"])
```

### 3.2. Adaptive RAG

Adaptive RAG는 쿼리 및 검색 결과에 따라 동적으로 전략을 조정합니다. 검색 결과가 불충분하면 쿼리를 재작성하거나 다른 검색 방법을 시도합니다.

#### 3.2.1. 핵심 개념

-   **검색 결과 평가**: 검색된 문서의 관련성과 품질을 평가하여 다음 단계를 결정합니다.
-   **동적 쿼리 재작성**: 검색 결과가 불충분할 경우 LLM을 활용하여 쿼리를 더 구체적으로 재작성합니다.
-   **검색 파라미터 조정**: 검색 전략과 파라미터를 동적으로 조정하여 더 나은 결과를 얻습니다.
-   **다중 검색 전략**: 시맨틱 검색, 키워드 검색, 하이브리드 검색 등 다양한 검색 전략을 활용합니다.

#### 3.2.2. 구현 예시

다음은 LangGraph를 활용한 Adaptive RAG 구현 예시입니다:

```python
from typing import TypedDict, List, Tuple, Literal, Optional, Dict, Any
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langgraph.graph import StateGraph, END

# 상태 정의
class AdaptiveRAGState(TypedDict):
    question: str
    rewritten_question: Optional[str]
    retrieved_documents: List[str]
    relevance_score: float
    search_strategy: Literal["semantic", "keyword", "hybrid"]
    final_answer: Optional[str]
    attempts: int

# 노드 함수 정의
def retrieve_documents(state: AdaptiveRAGState):
    """현재 검색 전략을 사용하여 문서를 검색합니다."""
    query = state.get("rewritten_question", state["question"])

    # 실제로는 벡터 DB와 연동하는 코드
    # 여기서는 간단한 예시만 제공
    documents = [
        f"Document for {query} using {state['search_strategy']} search"
    ]

    # 검색 품질 평가 시뮬레이션
    # 실제로는 LLM이나 다른 평가 방법을 사용
    if state["attempts"] > 2:
        relevance_score = 0.8  # 세 번째 시도에서는 좋은 결과가 나온다고 가정
    else:
        relevance_score = 0.4  # 처음 두 시도에서는 결과가 좋지 않다고 가정

    return {
        "retrieved_documents": documents,
        "relevance_score": relevance_score
    }

def evaluate_results(state: AdaptiveRAGState) -> Literal["rewrite_query", "change_strategy", "generate_answer"]:
    """검색 결과를 평가하고 다음 단계를 결정합니다."""
    # 검색 결과가 충분히 관련성이 높은지 확인
    if state["relevance_score"] >= 0.7:
        return "generate_answer"

    # 최대 시도 횟수 확인
    if state["attempts"] >= 3:
        return "generate_answer"  # 충분한 시도 후에는 그냥 진행

    # 쿼리 재작성 또는 검색 전략 변경 중 선택
    if state["attempts"] % 2 == 0:
        return "change_strategy"
    else:
        return "rewrite_query"

def rewrite_query(state: AdaptiveRAGState):
    """LLM을 사용하여 쿼리를 재작성합니다."""
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    response = llm.invoke(
        [HumanMessage(content=f"다음 질문을 더 구체적으로 재작성해주세요: {state['question']}")]
    )

    return {
        "rewritten_question": response.content,
        "attempts": state["attempts"] + 1
    }

def change_strategy(state: AdaptiveRAGState):
    """검색 전략을 변경합니다."""
    strategies = ["semantic", "keyword", "hybrid"]
    current_index = strategies.index(state["search_strategy"])
    next_index = (current_index + 1) % len(strategies)

    return {
        "search_strategy": strategies[next_index],
        "attempts": state["attempts"] + 1
    }

def generate_answer(state: AdaptiveRAGState):
    """검색된 문서를 바탕으로 최종 답변을 생성합니다."""
    llm = ChatOpenAI(model="gpt-4")

    context = "\n".join(state["retrieved_documents"])
    query = state.get("rewritten_question", state["question"])

    response = llm.invoke(
        [HumanMessage(content=f"컨텍스트: {context}\n\n질문: {query}")]
    )

    return {"final_answer": response.content}

# 그래프 구성
def build_adaptive_rag_graph():
    workflow = StateGraph(AdaptiveRAGState)

    # 노드 추가
    workflow.add_node("retrieve_documents", retrieve_documents)
    workflow.add_node("rewrite_query", rewrite_query)
    workflow.add_node("change_strategy", change_strategy)
    workflow.add_node("generate_answer", generate_answer)

    # 조건부 엣지 추가
    workflow.add_conditional_edges(
        "retrieve_documents",
        evaluate_results,
        {
            "rewrite_query": "rewrite_query",
            "change_strategy": "change_strategy",
            "generate_answer": "generate_answer"
        }
    )

    # 나머지 엣지 추가
    workflow.add_edge("rewrite_query", "retrieve_documents")
    workflow.add_edge("change_strategy", "retrieve_documents")
    workflow.add_edge("generate_answer", END)

    # 시작 노드 설정
    workflow.set_entry_point("retrieve_documents")

    return workflow.compile()

# 사용 예시
if __name__ == "__main__":
    graph = build_adaptive_rag_graph()
    result = graph.invoke({
        "question": "인공지능의 윤리적 문제는 무엇인가요?",
        "search_strategy": "semantic",
        "attempts": 0
    })
    print(result["final_answer"])
```

### 3.3. Collaborative RAG (CRAG)

Collaborative RAG는 여러 전문가 에이전트가 협력하여 복잡한 질문에 답변하는 접근 방식입니다. 각 에이전트는 서로 다른 전문 지식이나 역할을 가지고 공동으로 답변을 작성합니다.

#### 3.3.1. 핵심 개념

-   **다중 전문가 에이전트**: 서로 다른 전문 분야의 에이전트들이 협력하여 문제를 해결합니다.
-   **역할 기반 협업**: 각 에이전트는 특정 역할과 전문성을 가지고 있으며, 이를 바탕으로 분석을 제공합니다.
-   **상호 검토 및 개선**: 에이전트들은 서로의 분석을 검토하고 비평하여 더 나은 답변을 만들어냅니다.

#### 3.3.2. 구현 예시

다음은 LangGraph를 활용한 Collaborative RAG 구현 예시입니다:

````python
from typing import TypedDict, List, Dict, Optional, Any, Literal
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
import json

# 상태 정의
class CRAGState(TypedDict):
    question: str
    context: List[str]
    expert_analyses: Dict[str, str]
    critiques: Dict[str, Dict[str, str]]
    revised_analyses: Dict[str, str]
    final_answer: Optional[str]
    discussion_history: List[BaseMessage]

# 전문가 역할 정의
EXPERTS = {
    "technical_expert": "데이터베이스, 프로그래밍, 시스템 아키텍처에 관한 기술적 전문가",
    "business_expert": "비즈니스 프로세스, 산업 동향, 전략에 관한 전문가",
    "legal_expert": "데이터 규정, 개인정보보호, 법적 준수사항에 관한 전문가"
}

# 노드 함수 정의
def retrieve_context(state: CRAGState):
    """질문과 관련된 문서를 검색합니다."""
    # 실제로는 벡터 데이터베이스 검색 로직이 필요
    # 간단한 예시만 제공
    context = [
        "데이터베이스 마이그레이션 시 GDPR 준수 사항 문서",
        "클라우드 데이터베이스 비용 분석 보고서",
        "AWS에서 PostgreSQL로 마이그레이션하는 기술 가이드"
    ]
    return {"context": context}

def generate_expert_analyses(state: CRAGState):
    """각 전문가가 자신의 전문 분야에서 분석을 제공합니다."""
    llm = ChatOpenAI(model="gpt-4")
    expert_analyses = {}

    for expert_id, expert_desc in EXPERTS.items():
        response = llm.invoke([
            HumanMessage(content=f"""
            당신은 {expert_desc}입니다.

            질문: {state['question']}

            관련 정보:
            {state['context']}

            당신의 전문 분야에서 이 질문에 답변해주세요.
            """)
        ])
        expert_analyses[expert_id] = response.content

    return {"expert_analyses": expert_analyses}

def critique_analyses(state: CRAGState):
    """각 전문가가 다른 전문가의 분석을 비평합니다."""
    llm = ChatOpenAI(model="gpt-4")
    critiques = {}

    for reviewer_id, reviewer_desc in EXPERTS.items():
        critiques[reviewer_id] = {}

        for author_id, analysis in state["expert_analyses"].items():
            if reviewer_id != author_id:  # 자기 자신의 분석은 비평하지 않음
                response = llm.invoke([
                    HumanMessage(content=f"""
                    당신은 {reviewer_desc}입니다.

                    다음 분석을 검토하고 보완할 점을 지적해주세요:

                    질문: {state['question']}
                    분석: {analysis}

                    이 분석에서 누락된 중요한 관점이나 고려사항이 있나요?
                    """)
                ])
                critiques[reviewer_id][author_id] = response.content

    return {"critiques": critiques}

def revise_analyses(state: CRAGState):
    """각 전문가가 받은 비평을 바탕으로 자신의 분석을 수정합니다."""
    llm = ChatOpenAI(model="gpt-4")
    revised_analyses = {}

    for expert_id, expert_desc in EXPERTS.items():
        # 이 전문가에 대한 모든 비평 수집
        all_critiques = []
        for reviewer_id, critiques_by_reviewer in state["critiques"].items():
            if expert_id in critiques_by_reviewer:
                all_critiques.append(critiques_by_reviewer[expert_id])

        critiques_text = "\n\n".join(all_critiques)

        response = llm.invoke([
            HumanMessage(content=f"""
            당신은 {expert_desc}입니다.

            당신의 원래 분석:
            {state['expert_analyses'][expert_id]}

            받은 비평:
            {critiques_text}

            이 비평을 바탕으로 당신의 분석을 수정해주세요.
            """)
        ])
        revised_analyses[expert_id] = response.content

    return {"revised_analyses": revised_analyses}

def collaborative_discussion(state: CRAGState):
    """전문가들이 함께 토론하여 최종 답변을 만듭니다."""
    llm = ChatOpenAI(model="gpt-4")

    # 토론 기록 초기화 또는 가져오기
    history = state.get("discussion_history", [])

    if not history:
        # 토론 시작
        prompt = f"""
        다음은 '{state['question']}'에 대한 서로 다른 전문가들의 수정된 분석입니다:

        {json.dumps(state['revised_analyses'], indent=2, ensure_ascii=False)}

        이제 여러분은 협력하여 이 질문에 대한 종합적인 답변을 만들어야 합니다.
        먼저 각자 서로의 분석에서 가장 중요한 포인트가 무엇인지 논의해보세요.
        """
        history.append(HumanMessage(content=prompt))

        # 각 전문가가 한 번씩 발언
        for expert_id, expert_desc in EXPERTS.items():
            response = llm.invoke(history + [
                HumanMessage(content=f"당신은 {expert_desc}입니다. 토론에 참여해주세요.")
            ])
            history.append(AIMessage(content=f"[{expert_id}] {response.content}"))

    # 토론이 3턴 이상 진행되었으면 최종 답변 생성
    if len(history) >= 7:  # 초기 프롬프트 + 각 전문가 3명이 2번씩 = 7
        final_prompt = "지금까지의 토론을 바탕으로 질문에 대한 최종 답변을 작성해주세요."
        history.append(HumanMessage(content=final_prompt))

        response = llm.invoke(history)
        return {
            "discussion_history": history,
            "final_answer": response.content
        }
    else:
        # 토론 계속 진행
        response = llm.invoke(history + [
            HumanMessage(content="토론을 계속 진행해주세요. 서로의 관점을 통합하여 최종 답변을 향해 나아가세요.")
        ])
        history.append(AIMessage(content=response.content))

        return {
            "discussion_history": history
        }

def should_continue_discussion(state: CRAGState) -> Literal["continue", "finish"]:
    """토론을 계속할지 최종 답변을 생성할지 결정합니다."""
    if state.get("final_answer") is not None:
        return "finish"
    return "continue"

# 그래프 구성
def build_crag_graph():
    workflow = StateGraph(CRAGState)

    # 노드 추가
    workflow.add_node("retrieve_context", retrieve_context)
    workflow.add_node("generate_expert_analyses", generate_expert_analyses)
    workflow.add_node("critique_analyses", critique_analyses)
    workflow.add_node("revise_analyses", revise_analyses)
    workflow.add_node("collaborative_discussion", collaborative_discussion)

    # 엣지 추가
    workflow.add_edge("retrieve_context", "generate_expert_analyses")
    workflow.add_edge("generate_expert_analyses", "critique_analyses")
    workflow.add_edge("critique_analyses", "revise_analyses")
    workflow.add_edge("revise_analyses", "collaborative_discussion")

    # 조건부 엣지 추가
    workflow.add_conditional_edges(
        "collaborative_discussion",
        should_continue_discussion,
        {
            "continue": "collaborative_discussion",
            "finish": END
        }
    )

    # 시작 노드 설정
    workflow.set_entry_point("retrieve_context")

    return workflow.compile()

# 사용 예시
if __name__ == "__main__":
    graph = build_crag_graph()
    result = graph.invoke({
        "question": "우리 회사는 온프레미스 Oracle 데이터베이스를 AWS로 마이그레이션하려고 합니다. 기술적, 비즈니스적, 법적 관점에서 고려해야 할 사항은 무엇인가요?"
    })
    print(result["final_answer"])

### 3.4. Self-RAG

Self-RAG는 LLM이 자체적으로 검색 필요성을 판단하고, 검색 결과를 평가하며, 생성한 답변의 품질을 스스로 검증하는 접근 방식입니다.

#### 3.4.1. 핵심 개념

- **검색 필요성 평가**: LLM이 질문을 분석하여 외부 정보 검색이 필요한지 자체적으로 판단합니다.
- **검색 결과 관련성 평가**: 검색된 문서의 질문과의 관련성을 평가하여 유용한 정보를 선별합니다.
- **답변 신뢰도 평가**: 생성된 답변의 신뢰도를 스스로 평가하여 사용자에게 알려줍니다.
- **자체 인용 및 출처 표시**: 답변에 사용된 정보의 출처를 명확히 표시합니다.

#### 3.4.2. 구현 예시

다음은 LangGraph를 활용한 Self-RAG 구현 예시입니다:

```python
from typing import TypedDict, List, Dict, Optional, Literal, Any, Tuple
from langchain_core.messages import HumanMessage, AIMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langgraph.graph import StateGraph, END

# 상태 정의
class SelfRAGState(TypedDict):
    question: str
    needs_retrieval: bool
    retrieved_documents: List[Dict[str, Any]]
    relevant_documents: List[Dict[str, Any]]
    generated_answer: Optional[str]
    confidence_score: Optional[float]
    final_answer: Optional[str]
    citations: List[Dict[str, Any]]

# 노드 함수 정의
def assess_retrieval_need(state: SelfRAGState):
    """질문을 분석하여 검색이 필요한지 판단합니다."""
    llm = ChatOpenAI(model="gpt-3.5-turbo")

    response = llm.invoke([
        HumanMessage(content=f"""
        다음 질문을 분석하여 외부 정보가 필요한지 판단해주세요:

        "{state['question']}"

        이 질문에 답하기 위해 외부 정보 검색이 필요합니까?
        "예" 또는 "아니오"로만 답변해주세요.
        """)
    ])

    needs_retrieval = "예" in response.content

    return {"needs_retrieval": needs_retrieval}

def decide_next_step(state: SelfRAGState) -> Literal["retrieve", "generate_without_retrieval"]:
    """검색 필요성에 따라 다음 단계를 결정합니다."""
    if state["needs_retrieval"]:
        return "retrieve"
    else:
        return "generate_without_retrieval"

def retrieve_documents(state: SelfRAGState):
    """질문과 관련된 문서를 검색합니다."""
    # 실제로는 벡터 데이터베이스 검색 로직
    # 간단한 예시만 제공
    documents = [
        {"id": "doc1", "content": "LangGraph는 LangChain 기반의 프레임워크로, LLM 애플리케이션을 그래프 형태로 구현할 수 있습니다.", "source": "langchain_docs"},
        {"id": "doc2", "content": "RAG(Retrieval-Augmented Generation)는 LLM에 외부 정보를 제공하여 답변의 정확성과 신뢰성을 높이는 방법입니다.", "source": "research_paper"},
        {"id": "doc3", "content": "Self-RAG는 LLM이 스스로 검색 필요성을 판단하고 결과를 평가하는 접근 방식입니다.", "source": "blog_post"}
    ]

    return {"retrieved_documents": documents}

def assess_document_relevance(state: SelfRAGState):
    """검색된 문서의 관련성을 평가합니다."""
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    relevant_docs = []

    for doc in state["retrieved_documents"]:
        response = llm.invoke([
            HumanMessage(content=f"""
            다음 질문과 문서의 관련성을 평가해주세요:

            질문: "{state['question']}"
            문서: "{doc['content']}"

            이 문서는 질문에 답변하는 데 관련이 있습니까?
            0부터 10까지의 숫자로 관련성 점수를 매겨주세요.
            숫자만 답변해주세요.
            """)
        ])

        try:
            relevance_score = float(response.content.strip())
            if relevance_score >= 7:  # 관련성 임계값
                relevant_docs.append(doc)
        except ValueError:
            # 숫자 변환 실패 시 일단 포함
            relevant_docs.append(doc)

    return {"relevant_documents": relevant_docs}

def generate_answer_with_retrieval(state: SelfRAGState):
    """관련 문서를 바탕으로 답변을 생성합니다."""
    llm = ChatOpenAI(model="gpt-4")

    # 관련 문서가 없으면 빈 컨텍스트로 처리
    if not state["relevant_documents"]:
        context = "관련 정보를 찾지 못했습니다."
    else:
        context = "\n\n".join([doc["content"] for doc in state["relevant_documents"]])

    response = llm.invoke([
        HumanMessage(content=f"""
        다음 정보를 바탕으로 질문에 답변해주세요:

        질문: "{state['question']}"

        관련 정보:
        {context}

        답변을 생성할 때 관련 정보의 출처를 [1], [2]와 같은 형식으로 인용해주세요.
        """)
    ])

    return {"generated_answer": response.content}

def generate_answer_without_retrieval(state: SelfRAGState):
    """검색 없이 LLM 자체 지식으로 답변을 생성합니다."""
    llm = ChatOpenAI(model="gpt-4")

    response = llm.invoke([
        HumanMessage(content=f"""
        다음 질문에 당신의 지식만으로 답변해주세요:

        "{state['question']}"

        외부 정보 검색 없이 답변하되, 확실하지 않은 내용은 언급하지 마세요.
        """)
    ])

    return {"generated_answer": response.content}

def evaluate_confidence(state: SelfRAGState):
    """생성된 답변의 신뢰도를 평가합니다."""
    llm = ChatOpenAI(model="gpt-3.5-turbo")

    response = llm.invoke([
        HumanMessage(content=f"""
        다음 질문과 답변의 신뢰도를 평가해주세요:

        질문: "{state['question']}"
        답변: "{state['generated_answer']}"

        0부터 1까지의 숫자로 신뢰도 점수를 매겨주세요.
        숫자만 답변해주세요.
        """)
    ])

    try:
        confidence_score = float(response.content.strip())
    except ValueError:
        confidence_score = 0.5  # 기본값

    # 인용 추출 (간단한 구현)
    citations = []
    if state.get("relevant_documents"):
        for i, doc in enumerate(state["relevant_documents"]):
            if f"[{i+1}]" in state["generated_answer"]:
                citations.append({
                    "citation_id": i+1,
                    "source": doc["source"],
                    "content": doc["content"]
                })

    return {
        "confidence_score": confidence_score,
        "citations": citations
    }

def format_final_answer(state: SelfRAGState):
    """신뢰도와 인용을 포함하여 최종 답변을 포맷팅합니다."""
    answer = state["generated_answer"]

    # 낮은 신뢰도인 경우 면책 문구 추가
    if state["confidence_score"] < 0.7:
        disclaimer = "\n\n(참고: 이 답변은 제한된 정보를 기반으로 하며 추가 검증이 필요할 수 있습니다.)"
        answer += disclaimer

    # 인용 정보 추가
    if state["citations"]:
        answer += "\n\n출처:"
        for citation in state["citations"]:
            answer += f"\n[{citation['citation_id']}] {citation['source']}"

    return {"final_answer": answer}

# 그래프 구성
def build_self_rag_graph():
    workflow = StateGraph(SelfRAGState)

    # 노드 추가
    workflow.add_node("assess_retrieval_need", assess_retrieval_need)
    workflow.add_node("retrieve_documents", retrieve_documents)
    workflow.add_node("assess_document_relevance", assess_document_relevance)
    workflow.add_node("generate_answer_with_retrieval", generate_answer_with_retrieval)
    workflow.add_node("generate_answer_without_retrieval", generate_answer_without_retrieval)
    workflow.add_node("evaluate_confidence", evaluate_confidence)
    workflow.add_node("format_final_answer", format_final_answer)

    # 조건부 엣지 추가
    workflow.add_conditional_edges(
        "assess_retrieval_need",
        decide_next_step,
        {
            "retrieve": "retrieve_documents",
            "generate_without_retrieval": "generate_answer_without_retrieval"
        }
    )

    # 일반 엣지 추가
    workflow.add_edge("retrieve_documents", "assess_document_relevance")
    workflow.add_edge("assess_document_relevance", "generate_answer_with_retrieval")
    workflow.add_edge("generate_answer_with_retrieval", "evaluate_confidence")
    workflow.add_edge("generate_answer_without_retrieval", "evaluate_confidence")
    workflow.add_edge("evaluate_confidence", "format_final_answer")
    workflow.add_edge("format_final_answer", END)

    # 시작 노드 설정
    workflow.set_entry_point("assess_retrieval_need")

    return workflow.compile()

# 사용 예시
if __name__ == "__main__":
    graph = build_self_rag_graph()
    result = graph.invoke({
        "question": "LangGraph를 이용한 Self-RAG 구현 방법을 설명해주세요."
    })
    print(result["final_answer"])

## 4. Plan & Execute 패턴

Plan & Execute는 LLM 기반 에이전트가 복잡한 작업을 수행할 때 유용한 패턴으로, 먼저 전체 계획을 수립한 후 각 단계를 순차적으로 실행합니다. LangGraph를 사용하면 이 패턴을 효과적으로 구현할 수 있습니다.

### 4.1. 핵심 개념

-   **계획 단계(Planning)**: 전체 목표를 달성하기 위한 세부 단계들을 미리 계획합니다.
-   **실행 단계(Execution)**: 계획된 각 단계를 순차적으로 실행합니다.
-   **상태 관리**: 진행 상황과 중간 결과를 추적합니다.

### 4.2. 장점

-   **복잡한 작업을 관리 가능한 단계로 분해**: 큰 작업을 작은 단계로 나누어 처리합니다.
-   **장기적인 목표를 향해 일관되게 진행**: 전체 계획을 바탕으로 일관된 방향성을 유지합니다.
-   **각 단계의 결과를 다음 단계에 활용**: 단계별 결과를 다음 단계의 입력으로 활용합니다.
-   **실행 컨텍스트 관리**

### 4.3. 구현 예시

다음은 LangGraph를 활용한 Plan & Execute 패턴 구현 예시입니다:

```python
from typing import TypedDict, List, Dict, Optional, Any, Literal
from langchain_core.messages import HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
import json

# 상태 정의
class PlanExecuteState(TypedDict):
    goal: str
    plan: List[Dict[str, Any]]
    current_step: int
    completed_steps: List[Dict[str, Any]]
    final_result: Optional[str]
    error: Optional[str]

# 노드 함수 정의
def create_plan(state: PlanExecuteState):
    """목표를 달성하기 위한 계획을 생성합니다."""
    llm = ChatOpenAI(model="gpt-4")

    response = llm.invoke([
        HumanMessage(content=f"""
        다음 목표를 달성하기 위한 단계별 계획을 작성해주세요:

        목표: {state['goal']}

        각 단계는 다음과 같은 형식의 JSON으로 작성해주세요:
        {{
            "step": 단계 번호,
            "description": "단계 설명",
            "expected_output": "예상되는 출력",
            "dependencies": [이전 단계 번호들]
        }}

        JSON 배열 형태로 답변해주세요.
        """)
    ])

    try:
        plan = json.loads(response.content)
    except json.JSONDecodeError:
        # JSON 파싱 실패 시 기본 계획 생성
        plan = [
            {
                "step": 1,
                "description": "목표 분석",
                "expected_output": "목표의 주요 요구사항 목록",
                "dependencies": []
            },
            {
                "step": 2,
                "description": "필요한 리소스 확인",
                "expected_output": "필요한 리소스 목록",
                "dependencies": [1]
            },
            {
                "step": 3,
                "description": "실행 계획 수립",
                "expected_output": "상세 실행 계획",
                "dependencies": [1, 2]
            }
        ]

    return {"plan": plan, "current_step": 0}

def check_step_dependencies(state: PlanExecuteState) -> Literal["ready", "wait"]:
    """현재 단계의 의존성이 충족되었는지 확인합니다."""
    current_step = state["plan"][state["current_step"]]
    dependencies = current_step.get("dependencies", [])

    # 의존성 확인
    for dep in dependencies:
        if not any(step["step"] == dep for step in state["completed_steps"]):
            return "wait"

    return "ready"

def execute_step(state: PlanExecuteState):
    """현재 단계를 실행합니다."""
    llm = ChatOpenAI(model="gpt-4")
    current_step = state["plan"][state["current_step"]]

    # 이전 단계들의 결과 수집
    previous_results = "\n".join([
        f"단계 {step['step']} 결과: {step['result']}"
        for step in state["completed_steps"]
    ])

    response = llm.invoke([
        HumanMessage(content=f"""
        다음 단계를 실행해주세요:

        목표: {state['goal']}

        현재 단계:
        - 단계 번호: {current_step['step']}
        - 설명: {current_step['description']}
        - 예상 출력: {current_step['expected_output']}

        이전 단계 결과:
        {previous_results}

        이 단계를 실행하고 결과를 생성해주세요.
        """)
    ])

    # 단계 결과 저장
    completed_step = {
        "step": current_step["step"],
        "description": current_step["description"],
        "result": response.content
    }

    return {
        "completed_steps": state["completed_steps"] + [completed_step],
        "current_step": state["current_step"] + 1
    }

def check_completion(state: PlanExecuteState) -> Literal["continue", "finish"]:
    """모든 단계가 완료되었는지 확인합니다."""
    if state["current_step"] >= len(state["plan"]):
        return "finish"
    return "continue"

def generate_final_result(state: PlanExecuteState):
    """최종 결과를 생성합니다."""
    llm = ChatOpenAI(model="gpt-4")

    # 모든 단계 결과 수집
    all_results = "\n".join([
        f"단계 {step['step']}: {step['description']}\n결과: {step['result']}\n"
        for step in state["completed_steps"]
    ])

    response = llm.invoke([
        HumanMessage(content=f"""
        다음 목표와 단계별 결과를 바탕으로 최종 결과를 생성해주세요:

        목표: {state['goal']}

        단계별 결과:
        {all_results}

        모든 단계의 결과를 종합하여 최종 결과를 작성해주세요.
        """)
    ])

    return {"final_result": response.content}

# 그래프 구성
def build_plan_execute_graph():
    workflow = StateGraph(PlanExecuteState)

    # 노드 추가
    workflow.add_node("create_plan", create_plan)
    workflow.add_node("check_dependencies", check_step_dependencies)
    workflow.add_node("execute_step", execute_step)
    workflow.add_node("generate_final_result", generate_final_result)

    # 조건부 엣지 추가
    workflow.add_conditional_edges(
        "check_dependencies",
        check_step_dependencies,
        {
            "ready": "execute_step",
            "wait": "check_dependencies"
        }
    )

    workflow.add_conditional_edges(
        "execute_step",
        check_completion,
        {
            "continue": "check_dependencies",
            "finish": "generate_final_result"
        }
    )

    # 일반 엣지 추가
    workflow.add_edge("create_plan", "check_dependencies")
    workflow.add_edge("generate_final_result", END)

    # 시작 노드 설정
    workflow.set_entry_point("create_plan")

    return workflow.compile()

# 사용 예시
if __name__ == "__main__":
    graph = build_plan_execute_graph()
    result = graph.invoke({
        "goal": "웹 기반 문서 관리 시스템 설계 및 구현 계획 수립"
    })
    print(result["final_result"])

### 4.4. 활용 팁

Plan & Execute 패턴을 효과적으로 활용하기 위한 주요 팁은 다음과 같습니다:

#### 계획 단계 프롬프트 팁

1. **목표 및 툴 명시**
   - 목표를 정확하게 제시하고, 사용 가능한 툴 목록을 미리 제공합니다.
   - 이를 통해 LLM이 툴 활용을 고려한 실용적인 계획을 생성할 수 있습니다.

2. **계획 범위 제한**
   - 계획의 최대 단계 수를 제한하여 너무 세분화된 계획 생성을 방지합니다.
   - 각 단계의 범위를 적절히 조정하여 실행 가능한 계획을 생성합니다.

#### 실행 단계 프롬프트 팁

1. **중복 실행 방지**
   - 동일한 조건과 파라미터로 반복적인 툴 호출을 방지합니다.
   - 이전 실행 결과를 캐싱하거나 상태 관리하여 불필요한 실행을 최소화합니다.

2. **실행 컨텍스트 관리**
   - 각 단계 실행 시 필요한 컨텍스트 정보를 명확히 제공합니다.
   - 이전 단계의 결과를 효과적으로 활용하여 연속성을 유지합니다.

3. **에러 처리 및 복구**
   - 각 단계 실행 시 발생 가능한 에러 상황을 미리 고려합니다.
   - 에러 발생 시 적절한 복구 전략을 구현하여 전체 프로세스의 안정성을 높입니다.
````
