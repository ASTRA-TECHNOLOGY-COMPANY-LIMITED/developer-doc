# LLM and AI Agent Training Curriculum

## 1. Introduction to Large Language Models (LLMs)

Large Language Models (LLMs) are artificial intelligence models trained on vast amounts of text data to understand and generate human language. These models have revolutionized the field of Natural Language Processing (NLP) and have the following characteristics:

- Large-scale data learning: LLMs have billions of parameters and are trained on enormous amounts of text data collected from the internet.
- Self-supervised learning: Most LLMs are trained using self-supervised learning methods on unlabeled data. For example, by masking parts of a sentence and having the model predict the masked portions.
- Transfer learning: Pre-trained LLMs can be applied to various natural language processing tasks and can be further fine-tuned for specific tasks.
- Diverse applications: LLMs can perform a wide range of language-related tasks such as text generation, translation, summarization, and question-answering.

### 1.1. Early Language Models (1990s - Early 2000s)

**Models**: Statistical language models such as n-gram models

**Overview**: Simple models that predict the next word based on the frequency of previous words.

**Reasons**:
- Limited computing power favored simple statistical approaches.
- Lack of large-scale digital text data made training complex models difficult.
- These models were relatively easy to implement and interpret.

### 1.2. Neural Network-Based Models (Early 2010s)

**Models**: Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM) networks

**Overview**: Neural network structures that can process sequential data and remember previous information.

**Reasons**:
- Improved computing performance made training more complex models possible.
- Increased availability of large-scale digital text data.
- RNNs and LSTMs were effective in processing sequence data, capturing temporal dependencies in language well.
- However, they had limitations in learning long-range dependencies and difficulties in parallel processing.

### 1.3. The Emergence of Transformers (2017)

**Model**: Transformer architecture

**Overview**: A structure that can simultaneously consider all parts of the input sequence using an attention mechanism.

**Reasons**:
- Effectively solved the long-range dependency problem through the attention mechanism.
- Enabled parallel processing, greatly improving learning speed and efficiency.
- The encoder-decoder structure allowed flexible application to various NLP tasks.
- Unidirectional: Depends on previously generated tokens (e.g., "I am a ***")

### 1.4. The Advent of BERT and GPT (2018)

**Models**: BERT (Google), GPT (OpenAI)

**Overview**: Large-scale pre-trained models based on transformers, with BERT performing bidirectional learning and GPT performing unidirectional learning.

**Reasons**:
- The effectiveness of large-scale pre-training based on the transformer architecture was demonstrated.
- BERT's bidirectional learning greatly improved contextual understanding ability.
- GPT's autoregressive learning enabled natural text generation.
- Transfer learning allowed easy application to various downstream tasks.
- Bidirectional: Predicts randomly masked words ("The *** is bright today")

### 1.5. Scaling Up (2020 onwards)

**Models**: GPT-3, T5, PaLM, etc.

**Overview**: Ultra-large language models with hundreds of billions of parameters, capable of performing various tasks at near-human levels.

**Reasons**:
- Advancements in large-scale computing infrastructure made training of ultra-large models possible.
- Model performance improved dramatically by learning with more parameters and larger datasets.
- Emergence of abilities was observed as model size increased. That is, unexpected new abilities appeared when crossing certain thresholds.
- Few-shot learning ability improved, allowing models to perform various tasks with just a few examples.

## 2. How LLMs Work

### 2.1 Tokenization:

Divides input text into small units (tokens).
Example: "Hello there" → ["Hello", "there"]
Simple explanation: The process of breaking a sentence into words or meaningful parts.

### 2.2 Embedding:

Converts each token into a high-dimensional vector.
Example: "Hello" → [0.1, 0.3, -0.2, ...]
Simple explanation: The process of converting each word into a list of numbers that a computer can understand.

### 2.3 Self-Attention:

Calculates how each token relates to all other tokens.
Example: In "I ate an apple," "ate" has high relevance to "apple".
Simple explanation: The process of determining how importantly each word in a sentence is connected to other words.

### 2.4 Feed-Forward Network:

Enriches the representation of each token.
Simple explanation: The process of interpreting the meaning of each word more deeply.

### 2.5 Stacking Layers:

Repeats the above process multiple times to gain deeper understanding.
Simple explanation: Similar to reading a text multiple times to understand it more deeply.

### 2.6 Output Generation:

Uses the results from the final layer to predict the next token or generate desired output.
Example: Answering questions, completing sentences, etc.
Simple explanation: The process of creating an appropriate response based on the understood content.

## 3. LLM Fine-tuning

### 3.1. Understanding Fine-tuning

Fine-tuning is the process of further training a pre-trained large language model for specific tasks or domains.

- A form of transfer learning: Applies existing knowledge to new tasks.
- Efficiency: Saves time and resources compared to learning from scratch.
- Performance improvement: Can significantly improve model performance for specific tasks.
- Possible with little data: Utilizes the knowledge of large-scale pre-trained models, so it's effective with relatively little data.

### 3.2. Fine-tuning Techniques

#### 3.2.1. Full Fine-tuning

- Updates all model parameters.
- The most common method.

##### When to use
- When sufficient computational resources are available
- When there's a large target dataset
- When the source task and target task are significantly different

##### Advantages
- Highest potential for performance improvement
- Optimizes all parts of the model for the target task

##### Disadvantages
- High computational cost
- Requires a lot of storage space
- High risk of overfitting

#### 3.2.2. Partial Fine-tuning

- Updates only some layers of the model.
- Usually tunes upper layers while keeping lower layers fixed.

##### When to use
- When computational resources are limited
- When source and target tasks are similar
- When only a small amount of target data is available

##### Advantages
- More efficient than full fine-tuning
- Reduces risk of overfitting
- Faster learning and less memory usage

##### Disadvantages
- Performance improvement may be limited compared to full fine-tuning
- Can be difficult to decide which layers to tune

#### 3.2.3. Adapter Tuning

- Adds small adapter layers to the model structure and trains these.
- Does not change the original model parameters.

##### When to use
- When multiple tasks need to be processed with a single model
- When model size needs to be minimized
- When quick adaptation is needed

##### Advantages
- Very efficient parameter usage
- Easy to switch between adapters for different tasks
- Preserves the original model

##### Disadvantages
- Implementation can be complex
- Performance might be slightly lower than full fine-tuning

#### 3.2.4. Prompt Tuning

- Optimizes input prompts to adjust the model's output.
- Does not change model parameters.

##### When to use
- When model parameters cannot be changed
- When quick task switching is needed
- In zero-shot or few-shot learning scenarios

##### Advantages
- Requires very few additional parameters
- High flexibility as it doesn't change the model itself
- Allows quick switching between multiple tasks

##### Disadvantages
- Performance may be limited for complex tasks
- Designing effective prompts can be challenging

#### 3.2.5. LoRA (Low-Rank Adaptation)

- Efficiently tunes the model through low-dimensional updates.

##### When to use
- When large models need to be tuned efficiently
- When storage space and computational resources are limited
- When quick task switching is needed

##### Advantages
- Very efficient parameter usage
- Fast learning and low memory usage
- Can achieve performance nearly equivalent to the original model

##### Disadvantages
- Implementation can be complex
- May not be equally effective for all tasks

### 3.3. Fine-tuning Process

### 3.4. Evaluating Fine-tuned Models

#### Accuracy

Description: The proportion of correct predictions among total predictions
Purpose: Used to quickly grasp the overall performance of the model
Formula: (True Positives + True Negatives) / Total Predictions

#### Precision

- Description: The proportion of actual positives among those predicted as positive
- Purpose: Important when minimizing false positives is crucial
- Formula: True Positives / (True Positives + False Positives)

#### Recall

- Description: The proportion predicted as positive among actual positives
- Purpose: Important when minimizing false negatives is crucial
- Formula: True Positives / (True Positives + False Negatives)

#### F1 Score

- Description: The harmonic mean of precision and recall
- Purpose: Used to evaluate the balance between precision and recall
- Formula: 2 * (Precision * Recall) / (Precision + Recall)

#### Confusion Matrix

- Description: A table showing the relationship between predicted results and actual values
- Purpose: To visually grasp the model's prediction patterns and analyze error types

#### ROC Curve and AUC

- Description: A curve showing the relationship between True Positive Rate and False Positive Rate at various classification thresholds
- Purpose: Used to comprehensively evaluate the model's classification performance and compare with other models
- AUC (Area Under the Curve): The area under the ROC curve, with values closer to 1 indicating better performance

#### Error Analysis

- Description: The process of closely examining cases where the model made incorrect predictions
- Purpose: Used to identify the model's weaknesses and find areas for improvement
