# 1. Học máy (Machine Learning)

- Học dữ liệu để đưa ra công thức toán học mô tả dữ liệu
- y = w·x + b
- Điểm quan trọng là tìm ra w (trọng số)

# 2. Học sâu (Deep Learning)

- Mô phỏng cấu trúc não người để xây dựng mạng nơ-ron giả
- Cấu trúc như nơ-ron, synapse của não để kích hoạt nơ-ron trong tình huống cụ thể

## 2.1. Perceptron một lớp (Single-Layer Perceptron, SLP)

- Lớp: Lớp đầu vào, lớp đầu ra
- Nếu giá trị lớn hơn 0, kết quả là 1, ngược lại là 0
- y = 1 nếu w·x + b > 0, ngược lại 0
- Không thể giải quyết vấn đề XOR
- Không thể giải quyết vấn đề phi tuyến tính

## 2.2. Perceptron đa lớp (Multi-Layer Perceptron, MLP)

- Lớp: Lớp đầu vào, lớp ẩn, lớp đầu ra
- Có thể giải quyết vấn đề XOR
- Số lượng tham số: số lượng trọng số (weight) cần học
- Trong dữ liệu chiều cao như hình ảnh, có thể gặp vấn đề về lời nguyền của kích thước -> CNN
- Không thể xử lý dữ liệu đầu vào liên tiếp theo thời gian (khó áp dụng kết quả dự đoán trước đó, ví dụ: tạo câu) -> RNN

## 2.3. Mạng nơ-ron tích chập (Convolutional Neural Networks, CNN)

- Lớp: Lớp đầu vào, lớp ẩn (lớp tích chập, lớp gộp, lớp kết nối đầy đủ), lớp đầu ra
- Tạo ra bản đồ đặc trưng mới thông qua phép toán tích chập giữa lớp đầu vào (ma trận đầu vào) và bộ lọc (ma trận kích thước nhỏ)
- Sử dụng bộ lọc có thể giảm đáng kể số lượng tham số cần học

## 2.4. Mạng nơ-ron hồi quy (Recurrent Neural Network, RNN)

- Lớp: Lớp đầu vào, lớp ẩn (lớp hồi quy), lớp đầu ra
- Lớp hồi quy giữ mối quan hệ giữa các đầu vào theo thời gian (bộ nhớ)
- Mối quan hệ giữa các đầu vào được xem xét từ trái sang phải (đầu vào đầu tiên đến cuối cùng)
- Gặp vấn đề học dài hạn: vấn đề của RNN khi cố gắng nắm bắt mối quan hệ giữa các đầu vào xa xôi -> LSTM

## 2.5. Mạng nơ-ron hồi quy dài ngắn (Long Short-Term Memory, LSTM)

- Lớp: Lớp đầu vào, lớp ẩn (lớp hồi quy dài ngắn), lớp đầu ra
- Mô phỏng cấu trúc của tế bào thần kinh hồi quy dài ngắn để giải quyết vấn đề học dài hạn của RNN
- Cổng quên, cổng đầu vào, cổng đầu ra

## 2.6. Bộ chuyển đổi (Transformers)

- Lớp: Lớp đầu vào, lớp ẩn (encoder, decoder), lớp đầu ra
- Self-Attention: Tính toán trọng số giữa các mối quan hệ trong chuỗi đầu vào

## 2.7. BERT (Bidirectional Encoder Representations from Transformers)

- Lớp: Lớp đầu vào, lớp ẩn (encoder), lớp đầu ra
- Được đào tạo trước từ bộ dữ liệu ngôn ngữ lớn
  - Học cấu trúc và mẫu ngôn ngữ thông qua dữ liệu ngôn ngữ chung (như Wikipedia)
  - Masked Language Modeling (MLM): Học để tìm câu trả lời bằng cách che đi một phần từ trong câu, hiểu ngữ cảnh của câu
  - Next Sentence Prediction (NSP): Học xem câu thứ hai có phải là câu tiếp theo của câu đầu tiên không, hiểu mối quan hệ giữa các câu
- Tinh chỉnh
  - Đào tạo chuyên biệt cho một công việc cụ thể (phân tích cảm xúc, nhận dạng tên thực thể, trả lời câu hỏi, v.v.)
  - Cải thiện hiệu suất cho công việc cụ thể

## 2.8. GPT (Generative Pretrained Transformer)

- Lớp: Lớp đầu vào, lớp ẩn (decoder), lớp đầu ra
- Dự đoán từ tiếp theo dựa trên các từ trước đó
- Giống như BERT, đi qua quá trình đào tạo trước và tinh chỉnh
- Vấn đề: Khó phản ánh sự khác biệt tinh tế của ngữ cảnh. So với BERT là hai chiều, GPT chỉ tìm hiểu ngữ cảnh một chiều từ trái qua phải.
