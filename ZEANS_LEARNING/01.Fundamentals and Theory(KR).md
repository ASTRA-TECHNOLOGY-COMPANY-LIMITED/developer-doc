# 1. 머신러닝

- 데이터를 학습해서 데이터를 표현하는 수학 공식을 도출
- y = w·x + b
- 즉, w (가중치)를 찾아내는 것이 핵심
- 추가적으로, 머신러닝에는 비지도 학습, 반지도 학습, 강화학습 등 다양한 학습 방법론이 존재함.

# 2. 딥러닝

- 사람의 뇌구조를 모방해서 인공신경망을 구성
- 뇌의 뉴런, 시냅스처럼 특정 상황에 뉴런의 활성화 되도록 구성
- 활성화 함수로는 ReLU, Sigmoid, Tanh 등이 존재함.
- 신경망에서 가중치를 업데이트하는 주요 방법으로 Backpropagation(역전파)가 사용됨.

## 2.1. 단층 퍼셉트론(Single-Layer Perceptron, SLP)

- 레이어 : 입력층, 출력층
- 0 이상의 값인 경우, 결과는 1 반대의 경우 0
- y = 1 if w·x + b > 0, else 0
- XOR 문제의 해결이 불가능
- 즉, 비선형 문제의 해결이 불가능

## 2.2. 다층 퍼셉트론(Multi-Layer Perceptron, MLP)

- 레이어 : 입력층, 은닉층, 출력층
- XOR 문제의 해결이 가능해 짐
- 파라미터 갯수 : 학습해야 하는 weight(가중치)의 갯수
- 이미지 같은 고차원 데이터에서, 차원의 저주 문제가 발생 (파라미터 갯수가 너무 많음) -> CNN
- 시간적으로 연속된(Sequence) 입력 데이터의 처리는 불가능함 (이전 예측 결과를 반영하기 어려움, 예 : 문장 생성) -> RNN

## 2.3. 합성곱 신경망(Convolutional Neural Networks, CNN)

- 레이어 : 입력층, 은닉층 (합성곱 계층, 풀링 계층, 완전 연결 계층), 출력 층
- 입력층(입력 행렬)과 필터(작은 크기의 행렬)의 합성곱 연산을 통해 새로운 특징 맵을 생성
- 필터를 사용하게 되면 학습 파라미터의 갯수를 획기적으로 줄일 수 있음

## 2.4. 순환 신경망(Recurrent Neural Network, RNN)

- 레이어 : 입력층, 은닉층 (순환 계층), 출력층
- 순환 계층에서는 시간에 따른 입력 사이의 관계를 기억 (메모리)
- 시간에 따른 입력 사이의 관계는 다음 스텝의 순환 계층 또는 출력 층의 입력으로 전달
- 문제: 장기 의존성 문제(Long-term Dependency Problem) -> LSTM, GRU 등으로 해결

## 2.5. Sequence-to-Sequence(Seq2Seq)

- 레이어 : 입력층, 은닉층(인코더, 디코더), 출력층
- 인코더는 입력 시퀀스의 "요약" 또는 "표현"을 처리, 디코더로 전달되는 문맥 벡터
- 디코더는 문맥 벡터 + 토큰을 입력으로 출력 시퀀스 생성, 끝 토큰 또는 최대 출력 길이에 도달할 때까지 반복
- 어텐션 메커니즘(Attention Mechanism) : 필요한 부분에 "집중"하도록 하는 것입니다. 각 출력에 대해 가장 관련이 높은 입력 부분에 더욱 집중, 어텐션 가중치를 계산하고 입력으로 사용
- 크로스 어텐션 : 디코더가 입력 시퀀스의 각 위치에 얼마나 집중해야 하는 지를 결정, 예를들어 한글 : 영어

## 2.6. Transformer

- 레이어 : 입력층, 은닉층(인코더, 디코더), 출력층
- 인코더 : 셀프 어텐션(Self-Attention) 단어와 입력 시퀀스의 다른 부분 간의 상호 관계를 계산
- 인코더: 피드포워드 신경망(Feed Forward Neural Network) : 각 단어에 독립적으로 적용되는 신경망
- 디코더 : 셀프 어텐션(Self-Attention) 단어와 입력 시퀀스의 다른 부분 간의 상호 관계를 계산
- 디코더 : 인코더-디코더 어텐션(Encoder-Decoder Attention) 디코더가 인코더 출력의 각 위치에 얼마나 집중해야 하는지를 계산
- 셀프 어텐션(Self-Attention) : 입력 시퀀스의 관계간의 가중치를 계산

## 2.7. BERT(Bidirectional Encoder Representations from Transformers)

- 레이어 : 입력층, 은닉층(인코더), 출력층
- 대규모 언어 데이터셋에서 사전 학습
  - 일반적인 언어데이터(위키피디아 등)을 통해 언어의 구조와 패턴 학습
  - Masked Language Modeling (MLM): 문장의 일부단어 가리고 정답을 찾는 학습, 문장의 문맥 이해
  - Next Sentence Prediction (NSP): 두개의 문장을 기준으로, 두번째 문장이 첫번째 문장의 다음 문장인지 학습, 문장간 관계 이해
- 미세 조정(finetuning)
  - 특정 작업 (감정 분석, 개체명 인식, 질문 응답 등) 에 특화된 학습
  - 특정 작업에 대한 성능 향상

## 2.8. GPT (Generative Pretrained Transformer)

- 레이어 : 입력층, 은닉층(디코더), 출력층
- 이전 단어들을 기반으로 다음 단어를 예측
- BERT 와 마찬가지로 사전학습, 미세조정 과정을 거침
- 문제점: 문맥의 미묘한 차이를 반영하기 어렵다. BERT는 양방향인 반면, GPT는 단방향적으로 문맥을 파악하기 때문에

# 3. 출처

- OpenAI
- Wikipedia
- Various research papers and online resources
